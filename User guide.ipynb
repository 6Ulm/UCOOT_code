{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f317daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.1\n",
      "CUDA available: False\n",
      "CUDA version: 10.2\n",
      "CUDNN version: 7605\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print('Torch version: {}'.format(torch.__version__))\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "print('CUDA version: {}'.format(torch.version.cuda))\n",
    "print('CUDNN version: {}'.format(torch.backends.cudnn.version()))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7257cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.megawass import MegaWass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b123486",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa035d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate simulated data\n",
    "nx = 100\n",
    "dx = 30\n",
    "ny = 200\n",
    "dy = 20\n",
    "\n",
    "x = torch.rand(nx, dx).to(device)\n",
    "y = torch.rand(ny, dy).to(device)\n",
    "Cx = torch.cdist(x, x, p=2)**2\n",
    "Cy = torch.cdist(y, y, p=2)**2\n",
    "\n",
    "D_samp = torch.rand(nx, ny).to(device)\n",
    "D_feat = torch.rand(dx, dy).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b1c40",
   "metadata": {},
   "source": [
    "# UCOOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e519d3c",
   "metadata": {},
   "source": [
    "Given $2$ matrices of arbitrary size: $X_1 \\in \\mathbb R^{n_1 \\times d_1}$ and $X_2 \\in \\mathbb R^{n_2 \\times d_2}$, and $4$ corresponding histograms assigned to their rows and columns $\\mu_{n_1}, \\mu_{d_1}, \\mu_{n_2}$ and $\\mu_{d_2}$, the method $\\texttt{solver_fucoot}$ solves\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{FUCOOT}_{\\rho, \\lambda, \\varepsilon}(X_1, X_2) \n",
    "        &= \\inf_{\\substack{P_s \\in \\mathbb R^{n_1 \\times n_2}_{\\geq 0} \\\\ P_f \\in \\mathbb R^{d_1 \\times d_2}_{\\geq 0}}} \\text{func}(P_s, P_f) \n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "where the function\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{func}(P_s, P_f) \n",
    "        &= \\langle | X_1 - X_2 |^2, P_s \\otimes P_f \\rangle \n",
    "        + \\alpha_s \\langle D_s, P_s \\rangle + \\alpha_f \\langle D_f, P_f \\rangle \\\\\n",
    "        &+ \\rho_1 \\text{KL}(\\text{some function of $P_s$ and $P_f$} \\vert \\mu_{n_1} \\otimes \\mu_{d_1}) \n",
    "        + \\rho_2 \\text{KL}(\\text{some function of $P_s$ and $P_f$} \\vert \\mu_{n_2} \\otimes \\mu_{d_2})  \\\\\n",
    "        &+ \\varepsilon_s \\text{KL}(P_s | \\mu_{n_1} \\otimes \\mu_{n_2}) + \n",
    "        \\varepsilon_f \\text{KL}(P_f | \\mu_{d_1} \\otimes \\mu_{d_2}).\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "Here, the subscripts \"s\" and \"f\" mean sample and feature.\n",
    "\n",
    "Some notes on the input arguments:\n",
    "\n",
    "- By default, all histograms are uniform distributions, so just leave it as None if you don't want something else.\n",
    "\n",
    "- The input matrices $D_s \\in \\mathbb R^{n_1 \\times n_2}$ and $D_f \\in \\mathbb R^{d_1 \\times d_2}$ present prior knowledge (if available) on the sample and feature couplings, respectively. If they are not available, then just leave it as None.\n",
    "\n",
    "- The marginal relaxation parameters $\\rho_1$ and $\\rho_2$ can take any nonnegative values. It is also possible to use infinity value (by setting, e.g. $\\rho_1 = \\rho_2 = \\texttt{float(\"inf\")})$. In that case, you are doing (balanced) COOT and your epsilon **must** be **strictly positive**.\n",
    "\n",
    "- The regularisation parameters $\\varepsilon_s$ and $\\varepsilon_f$ can be **any** nonnegative values, **even zero**. **Important note**: if at least one of them is zero, then $\\rho_1$ and $\\rho_2$ must **not** contain infinity values. (because we use another algorithm to solve the case zero epsilon and it won't work with infinity value).\n",
    "\n",
    "- In case that you use zero epsilon, it may be desirable to increase argument $\\texttt{nits_uot}$ because the algorithm may converge not fast enough.\n",
    "\n",
    "- It is possible to trigger the early stopping if you see that the current and previous costs do not much differ. To do this, set your threshold via the argument $\\texttt{early_stopping_tol}$.\n",
    "\n",
    "- It is recommended that you set $\\texttt{verbose = True}$, so that you can see the evolution of costs. \n",
    "It is also possible to save the training cost by setting $\\texttt{log = True}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca0f0de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 1: 0.10952392220497131\n",
      "Cost at iteration 2: 0.10942281037569046\n",
      "Cost at iteration 3: 0.10932019352912903\n",
      "Cost at iteration 4: 0.10919800400733948\n",
      "Cost at iteration 5: 0.10904122143983841\n",
      "Cost at iteration 6: 0.10884430259466171\n",
      "Cost at iteration 7: 0.10861425846815109\n",
      "Cost at iteration 8: 0.10836313664913177\n",
      "Cost at iteration 9: 0.1080903708934784\n",
      "Cost at iteration 10: 0.10776674747467041\n",
      "Cost at iteration 11: 0.10730601847171783\n",
      "Cost at iteration 12: 0.10647690296173096\n",
      "Cost at iteration 13: 0.10486289858818054\n",
      "Cost at iteration 14: 0.10278765112161636\n",
      "Cost at iteration 15: 0.10104673355817795\n",
      "Cost at iteration 16: 0.09957534074783325\n",
      "Cost at iteration 17: 0.09793984144926071\n",
      "Cost at iteration 18: 0.09561534225940704\n",
      "Cost at iteration 19: 0.09287725389003754\n",
      "Cost at iteration 20: 0.09040720760822296\n",
      "Cost at iteration 21: 0.08824217319488525\n",
      "Cost at iteration 22: 0.08624337613582611\n",
      "Cost at iteration 23: 0.08485520631074905\n",
      "Cost at iteration 24: 0.08417405188083649\n",
      "Cost at iteration 25: 0.08386693894863129\n",
      "Cost at iteration 26: 0.08370990306138992\n",
      "Cost at iteration 27: 0.0836150050163269\n",
      "Cost at iteration 28: 0.08354892581701279\n",
      "Cost at iteration 29: 0.08349822461605072\n",
      "Cost at iteration 30: 0.08345663547515869\n",
      "Cost at iteration 31: 0.08342079818248749\n",
      "Cost at iteration 32: 0.08338819444179535\n",
      "Cost at iteration 33: 0.08335772156715393\n",
      "Cost at iteration 34: 0.08332899957895279\n",
      "Cost at iteration 35: 0.08330269902944565\n",
      "Cost at iteration 36: 0.08328048139810562\n",
      "Cost at iteration 37: 0.08326322585344315\n",
      "Cost at iteration 38: 0.08324984461069107\n",
      "Cost at iteration 39: 0.0832374095916748\n",
      "Cost at iteration 40: 0.08322261273860931\n",
      "Cost at iteration 41: 0.08320286124944687\n",
      "Cost at iteration 42: 0.08317749947309494\n",
      "Cost at iteration 43: 0.08314791321754456\n",
      "Cost at iteration 44: 0.08311638236045837\n",
      "Cost at iteration 45: 0.08308489620685577\n",
      "Cost at iteration 46: 0.08305422961711884\n",
      "Cost at iteration 47: 0.08302436769008636\n",
      "Cost at iteration 48: 0.08299502730369568\n",
      "Cost at iteration 49: 0.08296579122543335\n",
      "Cost at iteration 50: 0.08293692767620087\n",
      "Cost at iteration 51: 0.08290877938270569\n",
      "Cost at iteration 52: 0.08288189768791199\n",
      "Cost at iteration 53: 0.0828566700220108\n",
      "Cost at iteration 54: 0.08283351361751556\n",
      "Cost at iteration 55: 0.08281263709068298\n",
      "Cost at iteration 56: 0.08279428631067276\n",
      "Cost at iteration 57: 0.08277888596057892\n",
      "Cost at iteration 58: 0.08276674151420593\n",
      "Cost at iteration 59: 0.08275838196277618\n",
      "Cost at iteration 60: 0.08275371789932251\n",
      "Cost at iteration 61: 0.08275233954191208\n",
      "Cost at iteration 62: 0.08275361359119415\n",
      "Cost at iteration 63: 0.08275675773620605\n",
      "Cost at iteration 64: 0.0827609971165657\n",
      "Cost at iteration 65: 0.08276575803756714\n"
     ]
    }
   ],
   "source": [
    "rho = (1e-1, 1e-1) # use (float(\"inf\"), float(\"inf\")) if use COOT\n",
    "eps = (1e-2, 0)\n",
    "alpha = (1, 1) # optional, only care if D_s and / or D_f is available\n",
    "# D = (D_samp, D_feat) # optional, only care if D_s and / or D_f is available\n",
    "D = (None, None)\n",
    "\n",
    "megawass = MegaWass(nits_bcd=100, nits_uot=1000, tol_bcd=1e-6, tol_uot=1e-6, eval_bcd=1, eval_uot=20)\n",
    "\n",
    "(pi_samp, pi_feat), _, log_cost, log_ent_cost = megawass.solver_fucoot(\n",
    "        X=x,\n",
    "        Y=y,\n",
    "        rho=rho,\n",
    "        eps=eps,\n",
    "        alpha=alpha,\n",
    "        D=D,\n",
    "        log=True,\n",
    "        verbose=True,\n",
    "        early_stopping_tol=1e-6\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40b582",
   "metadata": {},
   "source": [
    "# UGW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000c5bf",
   "metadata": {},
   "source": [
    "Given $2$ square matrices of arbitrary size: $X_1 \\in \\mathbb R^{n_1 \\times n_1}$ and $X_2 \\in \\mathbb R^{n_2 \\times n_2}$, and $2$ corresponding histograms $\\mu_{n_1}$ and $\\mu_{n_2}$, the method $\\texttt{solver_fugw_simple}$ solves\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{FUGW}_{\\rho, \\lambda, \\varepsilon}(X_1, X_2) \n",
    "        &= \\inf_{P \\in \\mathbb R^{n_1 \\times n_2}_{\\geq 0}} \\text{func}(P) \n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "where the function\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\text{func}(P) \n",
    "        &= \\langle | X_1 - X_2 |^2, P \\otimes P \\rangle \n",
    "        + 2\\alpha \\; \\langle D, P \\rangle \\\\\n",
    "        &+ \\rho_1 \\text{KL}(\\text{some function of $P$} \\vert \\mu_{n_1} \\otimes \\mu_{n_1}) \n",
    "        + \\rho_2 \\text{KL}(\\text{some function of $P$} \\vert \\mu_{n_2} \\otimes \\mu_{n_2})  \\\\\n",
    "        &+ 2 \\varepsilon \\; \\text{KL}(P | \\mu_{n_1} \\otimes \\mu_{n_2}).\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Some notes on the input arguments: almost the same as above.\n",
    "\n",
    "- By default, all histograms are uniform distributions, so just leave it as None if you don't want something else.\n",
    "\n",
    "- The input matrice $D \\in \\mathbb R^{n_1 \\times n_2}$ presents prior knowledge (if available) on the sample couplings. If it is not available, then just leave it as None.\n",
    "\n",
    "- The marginal relaxation parameters $\\rho_1$ and $\\rho_2$ can take any nonnegative values. It is also possible to use infinity value (by setting, e.g. $\\rho_1 = \\rho_2 = \\texttt{float(\"inf\")})$. In that case, you are doing (balanced) GW and your epsilon **must** be **strictly positive**.\n",
    "\n",
    "- The regularisation parameter $\\varepsilon$ can be **any** nonnegative values, **even zero**. **Important note**: if it is zero, then $\\rho_1$ and $\\rho_2$ must **not** contain infinity values. (because we use another algorithm to solve the case zero epsilon and it won't work with infinity value).\n",
    "\n",
    "- In case that you use zero epsilon, it may be desirable to increase argument $\\texttt{nits_uot}$ because the algorithm may converge not fast enough.\n",
    "\n",
    "- It is possible to trigger the early stopping if you see that the current and previous costs do not much differ. To do this, set your threshold via the argument $\\texttt{early_stopping_tol}$.\n",
    "\n",
    "- It is recommended that you set $\\texttt{verbose = True}$, so that you can see the evolution of costs. \n",
    "It is also possible to save the training cost by setting $\\texttt{log = True}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e1839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 1: 0.19986319541931152\n",
      "Cost at iteration 2: 0.1990945041179657\n",
      "Cost at iteration 3: 0.198562890291214\n",
      "Cost at iteration 4: 0.19812944531440735\n",
      "Cost at iteration 5: 0.19785213470458984\n",
      "Cost at iteration 6: 0.19761922955513\n",
      "Cost at iteration 7: 0.1975046992301941\n",
      "Cost at iteration 8: 0.19747371971607208\n",
      "Cost at iteration 9: 0.1974550187587738\n",
      "Cost at iteration 10: 0.19744309782981873\n",
      "Cost at iteration 11: 0.19743773341178894\n"
     ]
    }
   ],
   "source": [
    "rho = (1e-1, 1e-1) # use (float(\"inf\"), float(\"inf\")) if use COOT\n",
    "eps = 1e-2\n",
    "alpha = 1 # optional, only care if D is available\n",
    "# D = D_samp # optional, only care if D is available\n",
    "D = None\n",
    "\n",
    "megawass = MegaWass(nits_bcd=100, nits_uot=1000, tol_bcd=1e-6, tol_uot=1e-6, eval_bcd=1, eval_uot=20)\n",
    "\n",
    "(pi_samp, pi_feat), _, log_cost, log_ent_cost = megawass.solver_fugw_simple(\n",
    "        X=Cx,\n",
    "        Y=Cy,\n",
    "        rho=rho,\n",
    "        eps=eps,\n",
    "        alpha=alpha,\n",
    "        D=D,\n",
    "        log=True,\n",
    "        verbose=True,\n",
    "        early_stopping_tol=1e-6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951f22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
